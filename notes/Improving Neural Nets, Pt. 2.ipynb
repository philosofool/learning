{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's continue learning about Neural Nets\n",
    "\n",
    "In part 1, we learned about othogonalization and some regularization algorithms.\n",
    "\n",
    "- Othogonalization refers to the idea that we separate the process of eliminating bias from the process of eliminating variance. That is, we find a network that performs very well on our training data first, then we implement regularization (or other processes) to improve dev set performance.\n",
    "\n",
    "- L2 regularization is the most common form. It refers to penalizing the cost function by the squared norm of the linear weights. \n",
    "\n",
    "- Drop-out regularization is another form. It refers to randomly selecting nodes in the network which will not be used during an iteration of training. The result is that no node relies heavily on any node, since it might be zero sometimes. \n",
    "\n",
    "See that notebook for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "This separates our training samples into groups and implements forward and backward propagation on the groups one at a time. \n",
    "\n",
    "```\n",
    "for t = 1,...,5000: ##suppose you have 5,000,000 examples\n",
    "    forward_prop X{t}: ##vectorize on a sub-set of those 5,000,000\n",
    "        Z[1] = W[1]X{t} + b[1]\n",
    "        A[1] = g[1](Z[1])\n",
    "        ...\n",
    "        A[l] = g[l](Z[l])\n",
    "    \n",
    "    \n",
    "    compute_cost:\n",
    "        J{t} = 1/1000 sum(Cost_func(label,prediction) + L2 regularization penalty\n",
    "    ##the 1000's are m, our training sizes, since we broke up the \n",
    "    ##training data into 5,000 samples of 1,000 examples each.\n",
    "    \n",
    "    complete back_prop, using X{t} and Y{t}\n",
    "```\n",
    "\n",
    "A single pass through the above pseudo-code is one **epoch**. Normally, training will require more than one epoch to minimize our cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Mini-batch Gradient Descent\n",
    "\n",
    "This process speeds up learning by using enough data to approximate the gradient in the whole batch while still taking adavantage of vectorization. If X{t} is small, we have a lot of looping and the gradient won't always be very close to the batch gradient. If X{t} is very large, we spend a lot of time computing a gradient for trivial increases in gradient precision compared with a medium sized X{t}.\n",
    "\n",
    "The optimal size (i.e., columns) for X{t} varies, but it is recommended that you chose a power of 2 between 5 (=32) and 8 (=512). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially weighted averages\n",
    "\n",
    "This is a process for smoothing noisy or eratic data where instantaneopus measurements tend to deviate from the mean value in the neighborhood. At least, that's sort of how I see it. Temperature is the example given in the video. How should we express temperature over the course of the year in a graph? Well, you don't want to take too many days of past data, since the temperature two months ago is probably colder or warmer than tomorrow will be. Let's suppose you only want to use recent data (not data from previous years.) A function like this would be a decent first bet:\n",
    "$$\n",
    "v(t) = .8v(t-1) + .2\\theta _t\n",
    "$$\n",
    "Which is just a weighted average. The general form for this is\n",
    "$$\n",
    "v(t) = \\beta v(t-1) + (1 - \\beta)\\theta _t\n",
    "$$\n",
    "This is function expresses the rolling temperature over a number of days\n",
    "$$\n",
    "\\textrm{no. days of the rolling average} = \\frac{1}{1 - \\beta}\n",
    "$$\n",
    "\n",
    "So if $\\beta = .8$ we're approximating the five day average.\n",
    "\n",
    "### Understanding a little more clearly\n",
    "\n",
    "v(t) is an exponentially decaying funciton. Notice that \n",
    "$$\n",
    "v(t) = .2\\theta _t +.8(.2\\theta_{t-1} + .8v(t-2)) \n",
    "$$\n",
    "So we're essentially multiplying each observed temperature day by the size of the function v(t) on that day, and v(t) rapidly approaches 0.\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "Very memory efficient:\n",
    "```\n",
    "v = 0\n",
    "update: \n",
    "    theta = get_next_theta\n",
    "    v = beta*v + (1-beta)*theta\n",
    "```\n",
    "This is much more memory efficient than keeping the last ten values of v and finding their average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias correction\n",
    "\n",
    "Exponentially weighted averages don't produce good estimates in the inital section of the series unless $\\beta$ is small. To correct this, we take $v_t$ and divide by $(1-\\beta^t)$, which converges to 1 rather quickly. So, for large $t$, we've very close to $v_t$, but for small t, $\\frac{v_t}{1-\\beta^t} \\approx (\\theta_1 + \\ldots + \\theta_t)/t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Momentum\n",
    "\n",
    "In one sentence:\n",
    "\n",
    "> Compute an exponentially weighted average of your gradient and use that to update your weights.\n",
    "\n",
    "This almost always converges faster.\n",
    "During each iteration, compute dW, db on current mini-batch and then compute ```VdW = beta*VdW + (1 - beta)dW``` and same for ```Vdb```. \n",
    "\n",
    "$$\n",
    "v_{dW} = \\beta v_{dW} + (1-\\beta)dW \\\\\n",
    "v_{db} = \\beta v_{db} + (1-\\beta)db \\\\\n",
    "W = W - \\alpha v_{dW}, b = b - \\alpha b_{db}\n",
    "$$\n",
    "\n",
    "In pratice, one doesn't usually need to tune $\\beta$, 0.9 is a strong value, but you can tune it if you want. Sometimes one sees the $1 - \\beta)$ term omitted, which is mathematically equivalent as long as you scale $\\alpha$ corespondingly, by $\\frac{1]{1 - \\beta}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Prop\n",
    "\n",
    "1. Compute dW and db on a mini batch\n",
    "1. Find SdW and Sdb, SdW = beta * SdW + (1-beta)dW^2, likewise for Sdb.\n",
    "1. Update W and b as W = W - alpha * dW/(sqrt(SdW)), likewise for b\n",
    "1. And just in case sqrt(SdW) is very close to zero, we add a tiny epsilon to prevent W from \"blowing up.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam: RMS Prop + Momentum\n",
    "\n",
    "This is one of the few optimization algorithms that's been shown to be consistently successful. \n",
    "\n",
    "1. Set VdW, SdW, Vdb, Sdb to 0.\n",
    "1. on each iteration, compute derivatives for mini-batch\n",
    "    1. VdW = beta_1 * VdW + (1-beta_1)dW, Vdb = (same)\n",
    "    1. SdW = beta_2 * VdW + (1-beat_2)dW^2\n",
    "    1. We do implement bias correction.\n",
    "    1. VdV = VdW / (1 - beta_1^t), and likewise for hte other three\n",
    "    1. Update W = W - alpha * VdW/(sqrt(SdW) + epsilon), etc. \n",
    "    \n",
    "### Hyperparameters with Adam\n",
    "\n",
    "- $\\alpha$ needs to be tuned.\n",
    "- $\\beta_1$ usually default as 0.9\n",
    "- $\\beta_2$ ysyally default as 0.999\n",
    "- $\\epsilon$ usually $10^8$. \n",
    "\n",
    "The name Adam is for Adaptive moment estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "The idea is to reduce the learning rate with each epoch. \n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{1 + \\textrm{decay rate} \\times \\textrm{epoch number}}\\alpha_0\n",
    "$$\n",
    "\n",
    "There are other function that people use (exponential decay, etc.)\n",
    "\n",
    "Ng sees learning rate decay as lower down his list of ways to tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2,\n",
       " 0.13333333333333333,\n",
       " 0.1,\n",
       " 0.08,\n",
       " 0.06666666666666667,\n",
       " 0.05714285714285715,\n",
       " 0.05,\n",
       " 0.044444444444444446,\n",
       " 0.04,\n",
       " 0.03636363636363637,\n",
       " 0.03333333333333333,\n",
       " 0.03076923076923077,\n",
       " 0.028571428571428574,\n",
       " 0.02666666666666667,\n",
       " 0.025,\n",
       " 0.023529411764705882,\n",
       " 0.022222222222222223,\n",
       " 0.021052631578947368,\n",
       " 0.02,\n",
       " 0.01904761904761905]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay_rate = .5\n",
    "[.2/(1+decay_rate*epoch_number) for epoch_number in range(0,20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local minima, local optima, etc\n",
    "\n",
    "Ng basically says \"we used to worry about that, but in high dimensional spaces, it doesn't really happen. Almost all regions where there's a zero gradient are saddle points.\"\n",
    "\n",
    "_Plateaus_ are a problem. A plateaus is just a large region where the derivative is close to zero. They slow things down a fair amount; optimizations like Adam help a bunch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "We have a lot to tune... alpha (L2 penalty weight), beta (exponential decay), mini-batch size, learning rate decay, # of layers, # of units in each layer. \n",
    "\n",
    "Some _hyperparameters_ are more important than others. Alpha is critical. We get another \"art of the science\" comment: it takes experience and not everyone agrees about how to optimize. \n",
    "\n",
    "My own take is that I wish there were more about how to select the number and size of hidden layers. It's obvious that these are somewhat dependent on the number of features that you have--is there a guideline here?\n",
    "\n",
    "### How to do it?\n",
    "\n",
    "He says __don't use a grid__! Why? Suppose you explore a 5 x 5 grid. Then you explore the same five values of hyperparameter one in five ways. If the second hyperparameter two has a small effect and the first has large one, you didn't get nearly as much info about H.P. one as you could have if you'd explored random pairs of values for them, which would get you twenty five different values for each of the two (but an equal number of combinations.)\n",
    "\n",
    "__Coarse to fine__. Start with a large space to find a region where parameters look good, then sample that region a second time.\n",
    "\n",
    "### Chosing scale\n",
    "\n",
    "Don't necessarily sample uniformly.\n",
    "- layers and unit count are a good choice for uniform sampling.\n",
    "- Search alpha on a log10 scale. \n",
    "\n",
    "> ```r = np.random.rand() * -4```\n",
    "\n",
    "> ```alpha = 10**r```\n",
    "\n",
    "- beta should be tuned in a logspace considering 1-beta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "We've seen that feature normalization is important to network learning. In fact, if we have a multi-layer network, normalization can occur at every layer to speed up learning.\n",
    "\n",
    "$$\n",
    "X \\xrightarrow{W^{[1]},b^{[1]}} Z^{[1]}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "It reduces covariant shift in the later layers of the network by making sure that the subsequent layer's mean and variance remain constant.\n",
    "\n",
    "#### NB \n",
    "\n",
    "Batch norm may have a regularization effect; larger batches will give a smaller effect. This is because each mini-batch has a mean and variance that is slightly different from the full set. \n",
    "\n",
    "#### Time for a framework\n",
    "\n",
    "It's worth understand how all this works, but he basically admits at this point that we have so much to tune and to do that we need a framework (like Tensorflow) to manage all this and that implementing the whole kit yourself is excessive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08,\n",
       "       1.e+09, 1.e+10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.dot(np.array([1,1,1]),np.array([[2,2],[1,2],[1,1]]))\n",
    "\n",
    "np.logspace(1,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
