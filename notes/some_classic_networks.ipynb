{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Case Studies in Neural Nets\n",
    "\n",
    "We look at some successful networks to gain insight on how to build NNs. See LeCun et al. (1998) \"Gradient Based Learning applied to document recognition\" in [Journal not mentioned].\n",
    "\n",
    "## LeNet-5 \n",
    "\n",
    "Build for recognizing 32x32 greyscale images of handwritten digits.\n",
    "\n",
    "32x32x1 --->\n",
    "\n",
    "CONV: 6 5x5 filters, stride 1\n",
    "28x28x6 --->\n",
    "\n",
    "AvPool: f=2, s=2\n",
    "14x14x6 --->\n",
    "\n",
    "CONV: 16 5x5 filters, stride 1\n",
    "10x10x16 --->\n",
    "\n",
    "AvPool: f=2, s=2\n",
    "5x5x16 --->\n",
    "\n",
    "FC : 400 ---> 120 ---> 84 --->\n",
    "SOFTMAX (10) ---> y-hat\n",
    "\n",
    "### Comments\n",
    "\n",
    "The use of average pooling is much less common today. (My speculation: averages tend to be pretty average and can washout, especially with f > 2, but representing the most extreme values tends to give a sense of where large changes happen.)\n",
    "\n",
    "The original used sigmoid and tanh activations, not relu. COmputational power limited the ability of the network and there's a fair amount of discussing how to apply filters to reduce complexity.\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## AlexNet\n",
    "\n",
    "Krizhevsky et al. (2012) ImageNet Clasification with deep convolutional neural networks.\n",
    "\n",
    "Input, 227x227x3 (RGB) --->\n",
    "\n",
    "CONV: 96 11x11, stride 4: 55x55x96 --->\n",
    "\n",
    "MAX-Pool: 3x3, stride =2: 27x27x96 --->\n",
    "\n",
    "CONV: 256 5x5 Same : 27x27x256 --->\n",
    "\n",
    "MaxPool 3x3, s = 2: 13x13,256--->\n",
    "\n",
    "CONV 3x3 Same 384 filders --->\n",
    "\n",
    "CONv 3x3 x 384 Same ---> (yes, they repeated that)\n",
    "\n",
    "CONV smae 3x3 x 256 same --->\n",
    "\n",
    "MaxPool 3x3, s = 2: 6x6x256 --->\n",
    "\n",
    "Flatten: 9216 x 1:\n",
    "\n",
    "FC 4096 --> 4096 --> Softmax(100)\n",
    "\n",
    "### Comments\n",
    "\n",
    "AlexNet has about 1000 times more parameters than LeNet. Relu was the activation. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 28, 28, 96)\n",
      "(None, 13, 13, 256)\n",
      "(None, 13, 13, 384)\n",
      "(None, 13, 13, 384)\n",
      "(None, 13, 13, 256)\n",
      "Model: \"AlexNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 227, 227, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 57, 57, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 28, 28, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 28, 28, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,322,314\n",
      "Trainable params: 58,322,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def alex_net():\n",
    "    '''\n",
    "    Returns an uncompiled AlexNet\n",
    "    '''\n",
    "    inputs = keras.Input(shape=(227,227,3))\n",
    "    x = keras.layers.Conv2D(96, kernel_size=11, strides=4, padding='same', activation='relu')(inputs)\n",
    "    x = keras.layers.MaxPool2D(pool_size=3, strides=2)(x)\n",
    "    print(x.shape)\n",
    "    x = keras.layers.Conv2D(256, kernel_size=5, strides=1, padding='same', activation='relu')(x)\n",
    "    x = keras.layers.MaxPool2D(pool_size=3, strides=2)(x)\n",
    "    print(x.shape)\n",
    "    x = keras.layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n",
    "    print(x.shape)\n",
    "    x = keras.layers.Conv2D(384, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n",
    "    print(x.shape)\n",
    "    x = keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n",
    "    print(x.shape)\n",
    "    x = keras.layers.MaxPool2D(pool_size=3, strides=2)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "    outputs = keras.layers.Dense(1000, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name = 'AlexNet')\n",
    "    return model\n",
    "alex_net().summary()"
   ]
  },
  {
   "source": [
    "## VGG-16\n",
    "\n",
    "This paper focused on using simple filters and pooling, rather than the more comlplex choices of the others. It's later (2015). The networks uses only 3x3,s=1,same filters and 2x2,s=2 maxpooling.\n",
    "\n",
    "Input 224x224x3 --->\n",
    "\n",
    "CONV(64)x2 ---> MAXPOOL --->\n",
    "\n",
    "CONV(128)x2 ---> MAXPOOL --->\n",
    "\n",
    "CONV(256)x3 + POOL ---> \n",
    "\n",
    "CONV(512)x3  + POOL --->\n",
    "\n",
    "CONV(512) + POOL --->\n",
    "\n",
    "Flatten ---> FC(4096)-->FC(4096)--->SOFTMAX\n",
    "\n",
    "### Comments\n",
    "\n",
    "There's a lot of simplicity here. The number of filters is roughly doubled each pass, with a small increase in the number of passes in each step.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"VGG-16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_13 (InputLayer)        [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nconv2d_53 (Conv2D)           (None, 224, 224, 64)      1792      \n_________________________________________________________________\nconv2d_54 (Conv2D)           (None, 224, 224, 64)      36928     \n_________________________________________________________________\nmax_pooling2d_28 (MaxPooling (None, 112, 112, 64)      0         \n_________________________________________________________________\nconv2d_55 (Conv2D)           (None, 112, 112, 128)     73856     \n_________________________________________________________________\nconv2d_56 (Conv2D)           (None, 112, 112, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_29 (MaxPooling (None, 56, 56, 128)       0         \n_________________________________________________________________\nconv2d_57 (Conv2D)           (None, 56, 56, 256)       295168    \n_________________________________________________________________\nconv2d_58 (Conv2D)           (None, 56, 56, 256)       590080    \n_________________________________________________________________\nconv2d_59 (Conv2D)           (None, 56, 56, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_30 (MaxPooling (None, 28, 28, 256)       0         \n_________________________________________________________________\nconv2d_60 (Conv2D)           (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nconv2d_61 (Conv2D)           (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nconv2d_62 (Conv2D)           (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_31 (MaxPooling (None, 14, 14, 512)       0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 100352)            0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 4096)              411045888 \n_________________________________________________________________\ndense_11 (Dense)             (None, 4096)              16781312  \n_________________________________________________________________\ndense_12 (Dense)             (None, 1000)              4097000   \n=================================================================\nTotal params: 439,559,464\nTrainable params: 439,559,464\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def conv(filters):\n",
    "    return keras.layers.Conv2D(filters=filters, kernel_size=3, activation='relu', strides=1, padding='same')\n",
    "\n",
    "def pool():\n",
    "    return keras.layers.MaxPool2D(pool_size=2, strides=2)\n",
    "\n",
    "def VGG_16():\n",
    "    inputs = keras.Input(shape=(224,224,3))\n",
    "    x = conv(64)(inputs)\n",
    "    x = conv(64)(x)\n",
    "    x = pool()(x)\n",
    "    x = conv(128)(x)\n",
    "    x = conv(128)(x)\n",
    "    x = pool()(x)\n",
    "    x = conv(256)(x)\n",
    "    x = conv(256)(x)\n",
    "    x = conv(256)(x)\n",
    "    x = pool()(x)\n",
    "    x = conv(512)(x)\n",
    "    x = conv(512)(x)\n",
    "    x = conv(512)(x)\n",
    "    x = pool()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "    x = keras.layers.Dense(1000, activation='softmax')(x)\n",
    "    return keras.Model(inputs, x, name='VGG-16')\n",
    "VGG_16().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}