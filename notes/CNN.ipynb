{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf-gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d7a5ec76fe5d4763a18ae5b40b958058b0b56ee7825baee741995c6e81379208"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "## A basic problem\n",
    "\n",
    "Suppose you have a 1,000x1,000 image; the first layer of a dense neural network would be a 3 billion element matrix. this is too big.\n",
    "\n",
    "## Let's detect edges instead\n",
    "\n",
    "Take a 6x6 image and perform a convolution on it.\n",
    "```\n",
    "[[8, 4, 9, 3, 3, 1],\n",
    "[6, 8, 0, 0, 2, 4],\n",
    "[1, 3, 3, 2, 4, 4],\n",
    "[2, 1, 2, 8, 5, 8],\n",
    "[9, 5, 5, 8, 1, 8],\n",
    "[4, 7, 3, 8, 4, 2]]\n",
    "```\n",
    " An edge detection filter of the form\n",
    "```\n",
    "[[ 1.,  0., -1.],\n",
    "[ 1.,  0., -1.],\n",
    "[ 1.,  0., -1.]]\n",
    "```\n",
    "Take the sum of the elementwise product of this filter imposed on each 3x3 section of the grid. The Convolutional output will be 4x4 with each elementwise product. Hence, the first cell of our convolution output will be 1*(8+6+1)+0*(4+8+3)+-1*(9+0+3).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How does this detect edges?\n",
    "\n",
    "If you look at an image with pixels like this:\n",
    "```\n",
    "[10., 10., 10.,  0.,  0.,  0.],\n",
    "[10., 10., 10.,  0.,  0.,  0.],\n",
    "[10., 10., 10.,  0.,  0.,  0.],\n",
    "[10., 10., 10.,  0.,  0.,  0.],\n",
    "[10., 10., 10.,  0.,  0.,  0.],\n",
    "[10., 10., 10.,  0.,  0.,  0.],\n",
    "```\n",
    "The middle 2 columns of the convolutional out put will be 30 and 0 on the edges. (You can verify this yourself.) The filter essentailly says \"there's an edge where we have dark pixels on one side and light ones on another.\" Note, that the numbers would be negative if right side were 10s and the left were ones."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1,0,-1 is arbitrary\n",
    "\n",
    "We could use a different matrix for our edge detector; it will have slightly different properties, which might be better or worse for our applicaiton. For example a Sobel filter would be:\n",
    "```\n",
    "[[ 1  0 -1]\n",
    " [ 2  0 -2]\n",
    " [ 1  0 -1]]\n",
    "```\n",
    "Which adds emphasis on the middle of the edge. Another possibility is Scharr:\n",
    "```\n",
    "[[  3   0  -3]\n",
    " [ 10   0 -10]\n",
    " [  3   0  -3]]\n",
    "```\n",
    "Which places more emphsis on the middle. \n",
    "\n",
    "## The **best** part\n",
    "\n",
    "Is that the value of the filter/kernel is a learnable parameter that can be trained by gradient descent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Padding\n",
    "\n",
    "We need to pad images because corners are used just once and edges 4 times while middle pixels get used 16 times. Also, a convolition reduces the dimensionality of our image. If we have a very deep network, we lose pixels.\n",
    "\n",
    "*Valid*: no Padding\n",
    "\n",
    "*Same*: output has dimension of input; this implies that padding = (f-1)/2, where f is the dimension of the filter.\n",
    "\n",
    "## Striding\n",
    "\n",
    "Stride is the number of steps taken before \"overlaying\" the filter. \n",
    "\n",
    "Once we adding padding (p) and stride (s) with filter (f), the dimension of the output is the floor of...\n",
    "$$dim = \\frac{n + 2p - f}{s}+1$$\n",
    "in each dimension."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3 dimensions\n",
    "\n",
    "Images are HxWxC dimensions, where C is the RGB chanel (usuall with 0-255 values).\n",
    "\n",
    "The 3D convolution filter is a 3D volume. To detect a vertical edge (light to dark):\n",
    "```\n",
    "[[[ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]]\n",
    "\n",
    " [[ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]]\n",
    "\n",
    " [[ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]]]\n",
    "  ```\n",
    "  To detect red edges:\n",
    "  ```\n",
    "[[[ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]\n",
    "  [ 1.  0. -1.]]\n",
    "\n",
    " [[ 0.  0. -0.]\n",
    "  [ 0.  0. -0.]\n",
    "  [ 0.  0. -0.]]\n",
    "\n",
    " [[ 0.  0. -0.]\n",
    "  [ 0.  0. -0.]\n",
    "  [ 0.  0. -0.]]]\n",
    "  ```\n",
    "  The convolution operation now overlays a _cubic array_ on the image, taking elemnetwise products and summing to get the output. N.B. _The output is 2D._ \n",
    "\n",
    "### Multiple Filters\n",
    "\n",
    "When we apply multiple filters, we stack the outputs of the filters. The shape will be \n",
    "\n",
    "$$dim = \\frac{n + 2p - f}{s}+1, \\frac{n + 2p - f}{s}+1, n_c$$\n",
    "\n",
    "where $n_c$ is the number of channels.\n",
    "\n",
    "When we train our model, we train each filter with a bias and an activation separately. \n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If we have 10 filters (3x3) then we have 28 x 10 paramters to learn. Each filter has 3x3 (=27 values) plus a bias correction to be learned. \n",
    "\n",
    "Notice that this is true regardless of the number of input features.\n",
    "\n",
    "### Notation Summary\n",
    "For a convoluation layer l:\n",
    "\n",
    "\n",
    "$f^{[l]}$ = filter size\n",
    "\n",
    "$p^{[l]}$ = padding\n",
    "\n",
    "$s^{[l]}$ = stride\n",
    "\n",
    "$n_c^{[l]}$ = # of filters\n",
    "\n",
    "Input: $n_H^{[l-1]}, n_W^{[l-1]}, n_c^{[l-1]}$\n",
    "\n",
    "Output: $n_H^{[l]}, n_W^{[l]}, n_c^{[l]}$\n",
    "\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## An example\n",
    "\n",
    "We can build a deep ConvNet in which there are several layers of convolution. Such a network might work like this:\n",
    "- Input Layer: A 39x39x3 layer.\n",
    "- Convolution 1: 3x3, stride 1, padding 0, 10 filters\n",
    "- Layer 1: 37x37x10 (this is the output from the above input + filter parameters.)\n",
    "- Convolution 2: 5x5, stride 2, p = 0, 20 filters.\n",
    "- Layer 2: 17x17x20\n",
    "- Conv 3: 5x5, stride 2, p=0, 40 filters\n",
    "- Layer 4: 7x7x40\n",
    "- Output: softmax/sigmoid layer.\n",
    "\n",
    "The trends here are common in computer vision: the layers tend to have shrinking dimensions with an increasing numner of filters and (hence) layer channels.\n",
    "\n",
    "### Layers in a ConvNet\n",
    "\n",
    "Usually, we find three types of layers in a Convolutional Network:\n",
    "- Convolution (CONV), which can be the only type\n",
    "- Pooling\n",
    "- Fully Connected\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pooling\n",
    "\n",
    "A pooling layer reduces the height and width dimensions by applying a filter in a simple way. A filter is applied by steps (as with a conv filter) and either the maximum value or the average value is the output. So, for example, a 4x4x2 layer is reduced to a 2x2x2 layer when we have a filter size of 2 and a step of 2; each index of the output would be a maximum in the 2x2 region it derives from.\n",
    "\n",
    "There are no paramters to learn with pooling. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# leNet-5-ish\n",
    "\n",
    "This is an example of a \"typical\" convolutional neural network:\n",
    "Inputs: 32x32x3\n",
    "Layer 1: CONV1, filter to 28x28x6, POOL1 pool to 14x14x6\n",
    "Layer 2: CONV2 filter to 10x10x16, POOL2 pool to 5x5x16\n",
    "Layer 3: Flatten to 400\n",
    "Layer 4: to Dense 120\n",
    "Layer 5: to Dense 84\n",
    "Output: Softmax 10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D\n",
    "\n",
    "## Similar to leNet5 example from Coursera\n",
    "\n",
    "inputs = keras.Input(shape=(28,28,1))\n",
    "x = keras.layers.experimental.preprocessing.RandomRotation(.06)(inputs)\n",
    "x = Conv2D(filters=8, kernel_size=5, activation='relu', name='CONV1')(x)\n",
    "x = MaxPooling2D(name='POOL1')(x)\n",
    "x = Conv2D(filters=16, kernel_size=5, activation='relu', name='CONV2')(x)\n",
    "x = MaxPooling2D(name='POOL2')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = Dense(120, activation='relu', name='FC3')(x)\n",
    "x = Dense(84, activation='relu', name='FC4')(x)\n",
    "outputs = Dense(10, activation='softmax', name='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../tf_keras/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).to_numpy().reshape((-1,28,28,1))\n",
    "y = df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', \n",
    "    loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nrandom_rotation_1 (RandomRot (None, 28, 28, 1)         0         \n_________________________________________________________________\nCONV1 (Conv2D)               (None, 24, 24, 8)         208       \n_________________________________________________________________\nPOOL1 (MaxPooling2D)         (None, 12, 12, 8)         0         \n_________________________________________________________________\nCONV2 (Conv2D)               (None, 8, 8, 16)          3216      \n_________________________________________________________________\nPOOL2 (MaxPooling2D)         (None, 4, 4, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 256)               0         \n_________________________________________________________________\nFC3 (Dense)                  (None, 120)               30840     \n_________________________________________________________________\nFC4 (Dense)                  (None, 84)                10164     \n_________________________________________________________________\nsoftmax (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 45,278\nTrainable params: 45,278\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "296/296 [==============================] - 1s 5ms/step - loss: 1.4852 - val_loss: 0.2782\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21391f91148>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "    validation_split=.1,\n",
    "    batch_size=128,\n",
    "    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "83/83 [==============================] - 1s 8ms/step - loss: 0.1261\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x214a615a7c8>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "model.fit(train_ds.batch(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}