{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbaseconda1fad7997e8f94ea293ffe5b4ecbfb0c4",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Titanic dataset classification, from Kaggle\n",
    "\n",
    "Seemed like an interesting way to learn a little about classification. One thing that's nice about a competition is that you can actually look at how well your algorithm performs relative to others. How else do you findout if you're making a strong model? \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports the essentials\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## open the dataset and explore\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "## We need to impute some stuff.\n",
    "print(df.isna().sum())\n",
    "\n",
    "## Cabin\n",
    "df['Cabin'].unique()\n",
    "\n",
    "## This is weird... the passenger fares are to very high precision.\n",
    "#print(df['Fare'].unique())\n",
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is weird... the passenger fares are to very high precision.\n",
    "## For families, they are sums.\n",
    "#print(df['Fare'].unique())\n",
    "df['Age'].describe()\n",
    "df[['Pclass','Fare']][df['Pclass'] == 1] ##Though correlated, these are not the same thing!\n",
    "pd.Series(df['Fare'].unique()).describe()\n",
    "df.sort_values(by='Ticket').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ticket'].value_counts().loc['347082']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineer(df):\n",
    "    '''\n",
    "    All feature engineering to the dataframe is done here and returned, as a copy. \n",
    "    '''\n",
    "    working_data = df.copy()\n",
    "    ages = SimpleImputer(strategy='mean', add_indicator=True).fit_transform(df[['Age',]])\n",
    "    working_data['Age'] = ages[:,0]\n",
    "    working_data['Age Unknown'] = ages[:,1]\n",
    "    working_data['Sex'] = df['Sex'].apply(lambda x: x == 'female')\n",
    "    working_data['Cabin'] = df['Cabin'].isna()\n",
    "    working_data['Family Size'] = df['SibSp'] + df['Parch']\n",
    "\n",
    "    ## The things below didn't improve the performance of my ensembles. \n",
    "    #working_data['Embarked'] = df['Embarked'].map({'S':0,'C':1,'Q':2}).astype('category')\n",
    "    #df['cabin group'] = df['Cabin'].apply(lambda x: x[0])\n",
    "    #working_data['Age Group'] = pd.cut(working_data['Age'], bins = [0,15,30,45,60,75,120], labels=False)\n",
    "    #working_data['is child'] = working_data['Age'].apply(lambda x: x < 18)\n",
    "\n",
    "    ## Data exploration suggests that many passengers buy one ticket for multiple persons.\n",
    "    ## We divid each fare by the number of people who have a matching ticket number and fill NaN with the mean.\n",
    "    working_data['Fare'] = working_data.apply(lambda x: x['Fare']/working_data['Ticket'].value_counts().loc[x['Ticket']], axis = 1)\n",
    "    working_data['Fare'].fillna(working_data['Fare'].mean(),inplace=True)\n",
    "\n",
    "    ## Cut the fare data into quartitles\n",
    "    working_data['Fare Group'] = pd.qcut(working_data['Fare'], 4, labels=False, )\n",
    "    \n",
    "    ## Select the data you want to use. Based on some experimentation below.\n",
    "    working_data = working_data.drop(['Name','Ticket','Embarked','Family Size','Cabin','Age Unknown','Fare'],axis=1)\n",
    "    return working_data\n",
    "working_data = feature_engineer(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(891, 8)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             PassengerId  Survived    Pclass       Sex       Age     SibSp  \\\n",
       "PassengerId     1.000000 -0.005007 -0.035144 -0.042939  0.033207 -0.057527   \n",
       "Survived       -0.005007  1.000000 -0.338481  0.543351 -0.069809 -0.035322   \n",
       "Pclass         -0.035144 -0.338481  1.000000 -0.131900 -0.331339  0.083081   \n",
       "Sex            -0.042939  0.543351 -0.131900  1.000000 -0.084153  0.114631   \n",
       "Age             0.033207 -0.069809 -0.331339 -0.084153  1.000000 -0.232625   \n",
       "SibSp          -0.057527 -0.035322  0.083081  0.114631 -0.232625  1.000000   \n",
       "Parch          -0.001652  0.081629  0.018443  0.245489 -0.179191  0.414838   \n",
       "Fare Group     -0.009839  0.305756 -0.812160  0.134794  0.304921 -0.013418   \n",
       "\n",
       "                Parch  Fare Group  \n",
       "PassengerId -0.001652   -0.009839  \n",
       "Survived     0.081629    0.305756  \n",
       "Pclass       0.018443   -0.812160  \n",
       "Sex          0.245489    0.134794  \n",
       "Age         -0.179191    0.304921  \n",
       "SibSp        0.414838   -0.013418  \n",
       "Parch        1.000000   -0.032146  \n",
       "Fare Group  -0.032146    1.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare Group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PassengerId</th>\n      <td>1.000000</td>\n      <td>-0.005007</td>\n      <td>-0.035144</td>\n      <td>-0.042939</td>\n      <td>0.033207</td>\n      <td>-0.057527</td>\n      <td>-0.001652</td>\n      <td>-0.009839</td>\n    </tr>\n    <tr>\n      <th>Survived</th>\n      <td>-0.005007</td>\n      <td>1.000000</td>\n      <td>-0.338481</td>\n      <td>0.543351</td>\n      <td>-0.069809</td>\n      <td>-0.035322</td>\n      <td>0.081629</td>\n      <td>0.305756</td>\n    </tr>\n    <tr>\n      <th>Pclass</th>\n      <td>-0.035144</td>\n      <td>-0.338481</td>\n      <td>1.000000</td>\n      <td>-0.131900</td>\n      <td>-0.331339</td>\n      <td>0.083081</td>\n      <td>0.018443</td>\n      <td>-0.812160</td>\n    </tr>\n    <tr>\n      <th>Sex</th>\n      <td>-0.042939</td>\n      <td>0.543351</td>\n      <td>-0.131900</td>\n      <td>1.000000</td>\n      <td>-0.084153</td>\n      <td>0.114631</td>\n      <td>0.245489</td>\n      <td>0.134794</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.033207</td>\n      <td>-0.069809</td>\n      <td>-0.331339</td>\n      <td>-0.084153</td>\n      <td>1.000000</td>\n      <td>-0.232625</td>\n      <td>-0.179191</td>\n      <td>0.304921</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>-0.057527</td>\n      <td>-0.035322</td>\n      <td>0.083081</td>\n      <td>0.114631</td>\n      <td>-0.232625</td>\n      <td>1.000000</td>\n      <td>0.414838</td>\n      <td>-0.013418</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>-0.001652</td>\n      <td>0.081629</td>\n      <td>0.018443</td>\n      <td>0.245489</td>\n      <td>-0.179191</td>\n      <td>0.414838</td>\n      <td>1.000000</td>\n      <td>-0.032146</td>\n    </tr>\n    <tr>\n      <th>Fare Group</th>\n      <td>-0.009839</td>\n      <td>0.305756</td>\n      <td>-0.812160</td>\n      <td>0.134794</td>\n      <td>0.304921</td>\n      <td>-0.013418</td>\n      <td>-0.032146</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "## by playing with the matching below, we can look to see if any variable stands out \n",
    "## as relevant to different groups.\n",
    "temp = working_data\n",
    "#temp = temp[(temp['Sex']==0)]\n",
    "#temp = temp[temp['SibSp'] == 0]\n",
    "print(temp.shape)\n",
    "temp.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(298, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(working_data.drop(['PassengerId','Survived'], axis=1),\n",
    "                    working_data['Survived'],\n",
    "                    train_size = .666)\n",
    "\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "source": [
    "## Let's establish a baseline\n",
    "\n",
    "A decision tree is quick and lets you know if what you've got."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7824620573355818\n0.7953020134228188\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth = 1, min_samples_leaf=3)\n",
    "simple = ['Sex']## use the feature that correlate most obviously with survival.\n",
    "tree_clf.fit(X_train[simple],y_train)\n",
    "print(tree_clf.score(X_train[simple],y_train))\n",
    "print(tree_clf.score(X_test[simple],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=30, max_depth = 6)\n",
    "#forest_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_eval_summary(clf,X=X_test,y=y_test, X_=X_train, y_=y_train):\n",
    "    print(\"Training score: {}\".format(clf.score(X_,y_)))\n",
    "    print(\"Test score: {}\".format(clf.score(X,y)))\n",
    "    print(\"Confusion:\")\n",
    "    print(confusion_matrix(y,clf.predict(X)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training score: 0.8482293423271501\nTest score: 0.8154362416107382\nConfusion:\n[[172  25]\n [ 30  71]]\nTraining score: 0.8549747048903878\nTest score: 0.8221476510067114\nConfusion:\n[[179  18]\n [ 35  66]]\nModel improvement over a simple model: 0.0201\n"
     ]
    }
   ],
   "source": [
    "## The following check whether adding a feature improves model performance.\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "diffs = []\n",
    "times = 5\n",
    "for i in range(times):\n",
    "    simple = ['Sex','Fare Group','Age']\n",
    "    forest_clf.fit(X_train[simple], y_train)\n",
    "    simpler = forest_clf.score(X_test[simple], y_test)\n",
    "    forest_clf.fit(X_train,y_train)\n",
    "    diffs.append(forest_clf.score(X_test, y_test) - simpler)\n",
    "    #print(diffs[-1], simpler)\n",
    "    #\n",
    "forest_clf.fit(X_train[simple], y_train)\n",
    "clf_eval_summary(forest_clf, X=X_test[simple], X_=X_train[simple])\n",
    "forest_clf.fit(X_train, y_train)\n",
    "clf_eval_summary(forest_clf)    \n",
    "print(\"Model improvement over a simple model: {:.4f}\".format(sum(diffs)/times))"
   ]
  },
  {
   "source": [
    "## Some validation and tuning of the Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8110810425865262"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "##Cross validate the model to see how it looks.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(forest_clf,X_train,y_train).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':[50,100],'max_depth' : [5,7,9]}\n",
    "forest_search = GridSearchCV(forest_clf, param_grid=parameters, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's worth noting that these estimators are mostly within a standard deviation or so\n",
    "## from each other in test performance. Without a bigger data set, it's hard to validate these parameters \n",
    "## against each other.\n",
    "\n",
    "best = forest_search.best_estimator_\n",
    "best"
   ]
  },
  {
   "source": [
    "## Other ensembles?\n",
    "\n",
    "I tried other models. Sometimes they outperformed the Random Forest, but they never improved the kaggle test set performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=2)"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gradient_clf = GradientBoostingClassifier(n_estimators=100, max_depth=2)\n",
    "gradient_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training score: 0.8696629213483146\nTest score: 0.8026905829596412\nConfusion:\n[[244  34]\n [ 54 114]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8089887640449438"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "clf_eval_summary(gradient_clf)\n",
    "cross_val_score(gradient_clf,X_train,y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svc = Pipeline(steps=[('scaler',StandardScaler()),('svc',SVC(C=1))])\n",
    "svc.fit(X_train,y_train)\n",
    "cross_val_score(svc, X_train, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'svc__C':[1,10,100],'svc__kernel':['rbf','poly']}\n",
    "\n",
    "svc_search = GridSearchCV(svc, param_grid=parameters, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_search.fit(X_train,y_train)\n",
    "svc_search.cv_results_"
   ]
  },
  {
   "source": [
    "## This was a moment\n",
    "\n",
    "I will let the cat out of the bag and say that a NN was the strongest model I made with only a little tuning. As you can see from the comments below, I was skeptical that a NN could outperform the other models I tried with a relatively small data set. In fact, I was so skeptical that I spent quite awhile feature engineering and tuning other models before I tried these. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('NN',\n",
       "                 MLPClassifier(alpha=0.01, hidden_layer_sizes=15,\n",
       "                               learning_rate_init=0.005))])"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "## I'm getting annoyed that my best classifier seems to top out around .785 on Kaggle and that improvements\n",
    "## on my dev set aren't translating to the test set. \n",
    "\n",
    "## I really don't think there's enough data to support a neural net.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## The parameters selected here are the results of the tuning that I did a few cells down.\n",
    "mlp_clf = Pipeline(steps=[('scaler',StandardScaler()),('NN', MLPClassifier(alpha=0.01, hidden_layer_sizes=15,\n",
    "                               learning_rate_init=0.005))])\n",
    "\n",
    "mlp_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('NN',\n",
       "                                        MLPClassifier(hidden_layer_sizes=10,\n",
       "                                                      learning_rate_init=0.03))]),\n",
       "             param_grid={'NN__alpha': [0.01, 0.03],\n",
       "                         'NN__hidden_layer_sizes': [15],\n",
       "                         'NN__learning_rate_init': [0.005, 0.01]},\n",
       "             return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "parameters = {'NN__learning_rate_init':[.005,.01], 'NN__hidden_layer_sizes': [10, 15], 'NN__alpha':[.003,.01,.03]}\n",
    "mlp_grid = GridSearchCV(mlp_clf, param_grid=parameters, return_train_score=True)\n",
    "mlp_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('NN',\n",
       "                 MLPClassifier(alpha=0.01, hidden_layer_sizes=15,\n",
       "                               learning_rate_init=0.005))])"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "mlp_grid.cv_results_\n",
    "mlp_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PassengerId    0\nPclass         0\nSex            0\nAge            0\nSibSp          0\nParch          0\nFare Group     0\ndtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     PassengerId  Pclass    Sex       Age  SibSp  Parch  Fare Group  Survived\n",
       "0            892       3  False  34.50000      0      0           0         0\n",
       "1            893       3   True  47.00000      1      0           0         0\n",
       "2            894       2  False  62.00000      0      0           1         0\n",
       "3            895       3  False  27.00000      0      0           1         0\n",
       "4            896       3   True  22.00000      1      1           1         1\n",
       "..           ...     ...    ...       ...    ...    ...         ...       ...\n",
       "413         1305       3  False  30.27259      0      0           1         0\n",
       "414         1306       1   True  39.00000      0      0           3         1\n",
       "415         1307       3  False  38.50000      0      0           0         0\n",
       "416         1308       3  False  30.27259      0      0           1         0\n",
       "417         1309       3  False  30.27259      1      1           2         0\n",
       "\n",
       "[418 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare Group</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>False</td>\n      <td>34.50000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>True</td>\n      <td>47.00000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>False</td>\n      <td>62.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>False</td>\n      <td>27.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>True</td>\n      <td>22.00000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>1305</td>\n      <td>3</td>\n      <td>False</td>\n      <td>30.27259</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>1306</td>\n      <td>1</td>\n      <td>True</td>\n      <td>39.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>415</th>\n      <td>1307</td>\n      <td>3</td>\n      <td>False</td>\n      <td>38.50000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>1308</td>\n      <td>3</td>\n      <td>False</td>\n      <td>30.27259</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>417</th>\n      <td>1309</td>\n      <td>3</td>\n      <td>False</td>\n      <td>30.27259</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows Ã— 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "results = feature_engineer(pd.read_csv('test.csv'))\n",
    "print(results.isna().sum()) ## double check NaN values in the output.\n",
    "\n",
    "mlp_clf.fit(X_train,y_train) ## makes sure the model is properly fitted. \n",
    "results['Survived'] = mlp_clf.predict(results.drop('PassengerId', axis=1))\n",
    "old_results = pd.read_csv('final_results.csv')\n",
    "results[['PassengerId','Survived']].set_index('PassengerId').to_csv('final_results.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "np.abs((results['Survived'] - old_results['Survived'])).sum() ## check to see if the new results are much different from the old."
   ]
  },
  {
   "source": [
    "## The MLP scored .79904\n",
    "\n",
    "As a number of people have mentioned in the discussion of the data, it's unfortunate that the leaderboard hasn't been reset. The actual Titanic data is publicly available. This means that there are lots of people with perfect scores. _But the data isn't really that powerful._ \n",
    "\n",
    "Anyway, we see below that this score places my in the 96th percentile of scores, once we get rid of obviously bad models and too good to be true ones."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaders = pd.read_csv('titanic-publicleaderboard.csv')\n",
    "leaders = leaders[(leaders['Score'] < .95) & (leaders['Score'] > .7)].sort_values(by='Score')\n",
    "leaders['Rank'] = leaders['Score'].rank(method='first', ascending=False)\n",
    "leaders['Percentile'] = (leaders.shape[0]-leaders['Rank'])/leaders.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        TeamId          TeamName       SubmissionDate    Score     Rank  \\\n",
       "10055  5610225      Mika Klinger  2020-10-04 20:03:42  0.70095  16115.0   \n",
       "10368  5615512     samran lushan  2020-10-15 15:13:58  0.70095  16116.0   \n",
       "8817   5579954  Puranjan Sojitra  2020-09-29 11:18:43  0.70095  16117.0   \n",
       "8639   5576800   shizuki.okumura  2020-09-26 11:09:45  0.70095  16118.0   \n",
       "501    1700801     Khalid Jeffal  2020-11-05 16:37:41  0.70095  16119.0   \n",
       "\n",
       "       Percentile  \n",
       "10055    0.000434  \n",
       "10368    0.000372  \n",
       "8817     0.000310  \n",
       "8639     0.000248  \n",
       "501      0.000186  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TeamId</th>\n      <th>TeamName</th>\n      <th>SubmissionDate</th>\n      <th>Score</th>\n      <th>Rank</th>\n      <th>Percentile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10055</th>\n      <td>5610225</td>\n      <td>Mika Klinger</td>\n      <td>2020-10-04 20:03:42</td>\n      <td>0.70095</td>\n      <td>16115.0</td>\n      <td>0.000434</td>\n    </tr>\n    <tr>\n      <th>10368</th>\n      <td>5615512</td>\n      <td>samran lushan</td>\n      <td>2020-10-15 15:13:58</td>\n      <td>0.70095</td>\n      <td>16116.0</td>\n      <td>0.000372</td>\n    </tr>\n    <tr>\n      <th>8817</th>\n      <td>5579954</td>\n      <td>Puranjan Sojitra</td>\n      <td>2020-09-29 11:18:43</td>\n      <td>0.70095</td>\n      <td>16117.0</td>\n      <td>0.000310</td>\n    </tr>\n    <tr>\n      <th>8639</th>\n      <td>5576800</td>\n      <td>shizuki.okumura</td>\n      <td>2020-09-26 11:09:45</td>\n      <td>0.70095</td>\n      <td>16118.0</td>\n      <td>0.000248</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>1700801</td>\n      <td>Khalid Jeffal</td>\n      <td>2020-11-05 16:37:41</td>\n      <td>0.70095</td>\n      <td>16119.0</td>\n      <td>0.000186</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "leaders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        TeamId   TeamName       SubmissionDate    Score   Rank  Percentile\n",
       "17222  5796517  sjlenhart  2020-11-10 21:30:40  0.79904  526.0    0.967374"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TeamId</th>\n      <th>TeamName</th>\n      <th>SubmissionDate</th>\n      <th>Score</th>\n      <th>Rank</th>\n      <th>Percentile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17222</th>\n      <td>5796517</td>\n      <td>sjlenhart</td>\n      <td>2020-11-10 21:30:40</td>\n      <td>0.79904</td>\n      <td>526.0</td>\n      <td>0.967374</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "leaders[leaders['TeamName'] == 'sjlenhart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}