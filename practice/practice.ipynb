{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Mapping, Iterable, Sequence, Generator\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, keras\n",
    "from keras import layers\n",
    "from keras.layers import StringLookup, IntegerLookup, Embedding, Normalization, Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.normpath(\"C:\\\\Users\\lenha\\Dropbox\\\\baseball\\\\all_seasons_hitters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df.to_csv(\"practice_baseball_data.csv\")\n",
    "def preprocess_data(df) -> pd.DataFrame:\n",
    "    \"\"\"Not really a part of practice. This is cleaning we would normally expect to be finished.\"\"\"\n",
    "    df = (\n",
    "        df.rename(columns={\"K%\": \"K\", \"BB%\": \"BB\"})\n",
    "        .assign(BB=lambda df: df['BB'].str.replace('%', \"\"), K=lambda df: df['K'].str.replace('%', \"\"))\n",
    "        .astype({\"BB\": 'float', 'K': 'float'})\n",
    "        .drop(columns=['xwOBA'])\n",
    "    )\n",
    "    df = df.merge(df.assign(Season=lambda df: df.Season + 1, label=lambda df: df.HR)[['Season', 'playerid','label']], on=['Season', 'playerid'], how='left').dropna()\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, seed=None, frac=None) -> tuple[pd.DataFrame, ...]:\n",
    "    test_size = min(int(len(df) * .2), 5000)\n",
    "    train, val = train_test_split(df, test_size=test_size, random_state=seed)\n",
    "    print(len(train), len(val))\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df: pd.DataFrame, shuffle: bool = True, batch_size: int = 256) -> tf.data.Dataset:\n",
    "    df = df.copy()\n",
    "    labels = df.pop('label')\n",
    "    df = {key: value.values[:, tf.newaxis] for key, value in df.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_ds(ds: tf.data.Dataset, name) -> tuple:\n",
    "    feature_ds = ds.map(lambda x, _: x[name])\n",
    "    dtype = feature_ds.element_spec.dtype\n",
    "    return feature_ds, dtype\n",
    "\n",
    "def make_embedding(data: tf.data.Dataset, name) -> Callable[..., Embedding]:\n",
    "    feature, dtype = extract_feature_ds(data, name)\n",
    "    if dtype == tf.string:\n",
    "        print(name)\n",
    "        lookup = StringLookup()\n",
    "    else:\n",
    "        lookup = IntegerLookup()\n",
    "    lookup.adapt(feature)\n",
    "    n_tokens = len(lookup.get_vocabulary())\n",
    "    # return lookup\n",
    "    return lambda x: Embedding(n_tokens, max(3, int(np.log(n_tokens))))(lookup(x))\n",
    "\n",
    "def make_normalization(data, name):\n",
    "    feature, dtype = extract_feature_ds(data, name)\n",
    "    encoder = Normalization()\n",
    "    encoder.adapt(feature)\n",
    "    return encoder\n",
    "\n",
    "def make_inputs_and_encoded_features(data, cat_cols) -> tuple[list, dict]:\n",
    "    all_inputs = []\n",
    "    encoded_features = {}\n",
    "    features = data.element_spec[0]\n",
    "    for name in features:\n",
    "        _input = keras.Input(shape=(1, ), name=name)\n",
    "        if name in cat_cols:\n",
    "            encoder = make_embedding(data, name)\n",
    "        else:\n",
    "            encoder = make_normalization(data, name)\n",
    "        print(name, encoder)\n",
    "        encoded_input = encoder(_input)\n",
    "        all_inputs.append(_input)\n",
    "        encoded_features[name] = encoded_input\n",
    "    return all_inputs, encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16822 4205\n"
     ]
    }
   ],
   "source": [
    "train, val = split_data(df, 42)\n",
    "train_ds = make_dataset(train)\n",
    "val_ds = make_dataset(val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Team', 'Season', 'playerid']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols =[col for col in df.select_dtypes('object').columns] + ['Season', 'playerid']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season <function make_embedding.<locals>.<lambda> at 0x000002998698BC40>\n",
      "Name\n",
      "Name <function make_embedding.<locals>.<lambda> at 0x000002998394CD60>\n",
      "Team\n",
      "Team <function make_embedding.<locals>.<lambda> at 0x000002998394E3E0>\n",
      "G <Normalization name=normalization_57, built=True>\n",
      "PA <Normalization name=normalization_58, built=True>\n",
      "HR <Normalization name=normalization_59, built=True>\n",
      "R <Normalization name=normalization_60, built=True>\n",
      "RBI <Normalization name=normalization_61, built=True>\n",
      "SB <Normalization name=normalization_62, built=True>\n",
      "BB <Normalization name=normalization_63, built=True>\n",
      "K <Normalization name=normalization_64, built=True>\n",
      "ISO <Normalization name=normalization_65, built=True>\n",
      "BABIP <Normalization name=normalization_66, built=True>\n",
      "AVG <Normalization name=normalization_67, built=True>\n",
      "OBP <Normalization name=normalization_68, built=True>\n",
      "SLG <Normalization name=normalization_69, built=True>\n",
      "wOBA <Normalization name=normalization_70, built=True>\n",
      "wRC+ <Normalization name=normalization_71, built=True>\n",
      "BsR <Normalization name=normalization_72, built=True>\n",
      "Off <Normalization name=normalization_73, built=True>\n",
      "Def <Normalization name=normalization_74, built=True>\n",
      "WAR <Normalization name=normalization_75, built=True>\n",
      "playerid <function make_embedding.<locals>.<lambda> at 0x00000299880B0A40>\n"
     ]
    }
   ],
   "source": [
    "all_inputs, all_features = make_inputs_and_encoded_features(train_ds, cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_topology(all_features, cat_cols):\n",
    "    flat_categorical = [keras.layers.Flatten()(all_features[x]) for x in cat_cols]\n",
    "    non_cat = [all_features[key] for key in all_features if key not in cat_cols]\n",
    "    x = keras.layers.Concatenate()(flat_categorical + non_cat)\n",
    "    x = Dense(8, activation='relu')(x)\n",
    "    x = Dense(1, 'sigmoid')(x)\n",
    "    return x\n",
    "\n",
    "model = keras.Model(all_inputs, model_topology(all_features, cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inputs to a layer should be tensors. Got '<_MapDataset element_spec=TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)>' (of type <class 'tensorflow.python.data.ops.map_op._MapDataset'>) as input for layer 'functional_4'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lenha\\Repos\\learning\\practice\\practice.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenha/Repos/learning/practice/practice.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (name, encoder) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(all_features\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenha/Repos/learning/practice/practice.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     feature \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, _: x[name])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenha/Repos/learning/practice/practice.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     keras\u001b[39m.\u001b[39;49mModel(all_inputs[i], encoder)(feature)\n",
      "File \u001b[1;32mc:\\Users\\lenha\\Repos\\learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\lenha\\Repos\\learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:176\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    177\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInputs to a layer should be tensors. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m) as input for layer \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    181\u001b[0m shape \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mstandardize_shape(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    182\u001b[0m ndim \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(shape)\n",
      "\u001b[1;31mValueError\u001b[0m: Inputs to a layer should be tensors. Got '<_MapDataset element_spec=TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)>' (of type <class 'tensorflow.python.data.ops.map_op._MapDataset'>) as input for layer 'functional_4'."
     ]
    }
   ],
   "source": [
    "for i, (name, encoder) in enumerate(all_features.items()):\n",
    "    feature = train_ds.map(lambda x, _: x[name])\n",
    "    keras.Model(all_inputs[i], encoder)(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_embedding(train_ds, 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inputs(df: pd.DataFrame, as_cat=set()) -> dict:\n",
    "    encoders = {}\n",
    "    for col in df:\n",
    "\n",
    "        name=f\"{col}_encoder\"\n",
    "        print(name)\n",
    "        if df[col].dtype == 'object' or col in as_cat:\n",
    "            # print(df[col])\n",
    "            print(name)\n",
    "            if isinstance(df[col][0], str):\n",
    "                lookup_layer = keras.layers.StringLookup\n",
    "                print(col, \" is string type.\")\n",
    "            else:\n",
    "                print(col, \"is int type.\")\n",
    "                lookup_layer = keras.layers.IntegerLookup\n",
    "            values =  df[col].unique().tolist()\n",
    "            lookup = lookup_layer(vocabulary=values)\n",
    "            embedding = keras.layers.Embedding(len(values), int(np.log(len(values))))\n",
    "            # FIXME: just pass an input to the damned sequence.\n",
    "            # and flatten\n",
    "            # print(lookup)\n",
    "            flatten = layers.Flatten()\n",
    "            encoder = lambda x: flatten(embedding(lookup_layer(x), name=f\"{col}_embedding\"))\n",
    "            encoders[col] = encoder\n",
    "        else:\n",
    "            # print(col, \" is numeric.\")\n",
    "            encoders[col] = keras.layers.Normalization(name=name)\n",
    "        # encoders[col] = map_to_encoder(args)\n",
    "    return encoders\n",
    "\n",
    "def make_inputs(df, as_cat=set()) -> dict:\n",
    "    encoders = make_encoders(df, as_cat)\n",
    "    return {k: encoder(keras.Input(shape=(1,), name=k)) for k, encoder in encoders.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> tuple[pd.DataFrame, ...]:\n",
    "    df = df.copy()\n",
    "    test_size = int(min(len(df) / 5, 5_000))\n",
    "    target = df.pop('target')\n",
    "    split_data = train_test_split(df, target, test_size=test_size)\n",
    "    return [dict(df) for df in split_data]\n",
    "\n",
    "x_train, x_val, y_train, y_val = prepare_data(df.drop(columns='Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StringLookup(vocabulary=['SEA'])(['se', 'SEA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_encoders(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = make_inputs(x_train, as_cat=['playerid', 'Season'])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(inputs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Concatenate()(list(inputs.values()))\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(f\"    {i}: ( ),\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
