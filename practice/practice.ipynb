{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Mapping, Iterable, Sequence, Generator\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf, keras\n",
    "from keras import layers\n",
    "from keras.layers import StringLookup, IntegerLookup, Embedding, Normalization, Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.normpath(\"practice_baseball_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "\n",
    "def make_label(df) -> pd.Series:\n",
    "    above_average_for_season = df.groupby(['Season'])['HR'].transform(np.mean)\n",
    "    return (df.HR > above_average_for_season).astype(int)\n",
    "\n",
    "\n",
    "def preprocess_data(df) -> pd.DataFrame:\n",
    "    \"\"\"Not really a part of practice. This is cleaning we would normally expect to be finished.\"\"\"\n",
    "    df = (\n",
    "        df.rename(columns={\"K%\": \"K\", \"BB%\": \"BB\"})\n",
    "        .assign(BB=lambda df: df['BB'].str.replace('%', \"\"), K=lambda df: df['K'].str.replace('%', \"\"))\n",
    "        .astype({\"BB\": 'float', 'K': 'float'})\n",
    "        .drop(columns=['xwOBA'])\n",
    "    )\n",
    "    df = df.merge(df.assign(Season=lambda df: df.Season + 1, label=make_label)[['Season', 'playerid','label']], on=['Season', 'playerid'], how='left').dropna()\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)\n",
    "# df = df.drop(columns=['Name', \"Team\"])\n",
    "selected = ['label'] + [\"Season\", \"HR\", \"K\", \"BB\", \"PA\", \"Team\"]\n",
    "df = df[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, seed=None, frac=None) -> tuple[pd.DataFrame, ...]:\n",
    "    test_size = min(int(len(df) * .2), 5000)\n",
    "    train, val = train_test_split(df, test_size=test_size, random_state=seed)\n",
    "    print(len(train), len(val))\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df: pd.DataFrame, shuffle: bool = True, batch_size: int = 1024) -> tf.data.Dataset:\n",
    "    \"\"\"Turn dataframe into tensorflow dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "    labels = df.pop('label')\n",
    "    df = {key: value.values[:, tf.newaxis] for key, value in df.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "\n",
    "def extract_feature_ds(ds: keras.Input, name) -> tuple:\n",
    "    \"\"\"Return a dataset of a single feature. Helpful for adpapting lookups.\"\"\"\n",
    "    feature_ds = ds.map(lambda x, _: x[name])\n",
    "    dtype = feature_ds.element_spec.dtype\n",
    "    return feature_ds, dtype\n",
    "\n",
    "def make_embedding(data: tf.data.Dataset, name) -> Callable[..., Embedding]:\n",
    "    feature, dtype = extract_feature_ds(data, name)\n",
    "    if dtype == tf.string:\n",
    "        lookup = StringLookup()\n",
    "    else:\n",
    "        lookup = IntegerLookup()\n",
    "    lookup.adapt(feature)\n",
    "    n_tokens = len(lookup.get_vocabulary())\n",
    "    output_dim = int(np.log(n_tokens))\n",
    "    embedding = Embedding(n_tokens, max(1, output_dim))\n",
    "    # return lookup\n",
    "    return lambda x: embedding(lookup(x))\n",
    "\n",
    "def make_normalization(data, name):\n",
    "    feature, dtype = extract_feature_ds(data, name)\n",
    "    encoder = Normalization()\n",
    "    encoder.adapt(feature)\n",
    "    return encoder\n",
    "\n",
    "def make_inputs_and_encoded_features(data, cat_cols) -> tuple[list, dict]:\n",
    "    all_inputs = []\n",
    "    encoded_features = {}\n",
    "    features = data.element_spec[0]\n",
    "    for name, spec in features.items():\n",
    "        # if name in [\"Name\", \"Team\"]:\n",
    "        #     continue\n",
    "        _input = keras.Input(shape=(1, ), name=name, dtype=spec.dtype)\n",
    "        if name in cat_cols:\n",
    "            encoder = make_embedding(data, name)\n",
    "        else:\n",
    "            encoder = make_normalization(data, name)\n",
    "        # print(name, encoder)\n",
    "        encoded_input = encoder(_input)\n",
    "        all_inputs.append(_input)\n",
    "        encoded_features[name] = encoded_input\n",
    "    return all_inputs, encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_inputs_and_encoded_features():\n",
    "    df = pd.DataFrame({\n",
    "        'a': list('abcdefgh'),\n",
    "        'label': [i % 2 for i in range(8)]\n",
    "    })\n",
    "    ds = make_dataset(df)\n",
    "\n",
    "    feature_ds = ds.map(lambda x, _: x['a'])\n",
    "    lookup = StringLookup()\n",
    "    lookup.adapt(feature_ds)\n",
    "    input_dims = len(lookup.get_vocabulary())\n",
    "    embedding = lambda x: Embedding(input_dim=input_dims, output_dim=2)(lookup(x))\n",
    "\n",
    "    # embedding works as expected on a numpy array.\n",
    "    assert embedding(df['a'].values).shape == (8, 2), \"This embedding doesn't work on raw data.\"\n",
    "\n",
    "    inputs = keras.Input((1,), name='a', dtype=tf.string)\n",
    "    # x = embedding(inputs)\n",
    "    x = Embedding(input_dim=input_dims, output_dim=2)(lookup(inputs))\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    out = Dense(1, 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs, out)\n",
    "    model({'a': df.a.values})\n",
    "    model.summary()\n",
    "    model.predict(ds)  # !! fails with error\n",
    "\n",
    "\n",
    "test_make_inputs_and_encoded_features()\n",
    "\n",
    "# emb(list('abczxy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_cols = [col for col in df.select_dtypes('object').columns] + ['Season', 'playerid']\n",
    "\n",
    "train, val = split_data(df, 42)\n",
    "train_ds = make_dataset(train)\n",
    "val_ds = make_dataset(val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs, all_features = make_inputs_and_encoded_features(train_ds, cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_topology(all_features, cat_cols):\n",
    "    flat_categorical = [keras.layers.Flatten()(all_features[x]) for x in cat_cols if x in all_features]\n",
    "    non_cat = [all_features[key] for key in all_features if key not in cat_cols]\n",
    "    x = keras.layers.Concatenate()(flat_categorical + non_cat)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    return x\n",
    "\n",
    "model = keras.Model(all_inputs, model_topology(all_features, cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(learning_rate=.0031), keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(val_ds, epochs = 30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "y_true = val['label'].values.astype(float)\n",
    "y_hat = model.predict(val_ds).reshape(-1)\n",
    "confusion_matrix(y_true, y_hat > .5)\n",
    "CalibrationDisplay.from_predictions(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
