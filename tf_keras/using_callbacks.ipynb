{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow",
   "display_name": "Python 3.8 (tensorflow)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using Callbacks\n",
    "\n",
    "Callbacks are an integral part of Keras. Callbacks are used to get performance information, log progress, halt in the event of errors, tune parameters, save model state (in case of crash, etc.), finish training once loss is minimized. The list goes on.\n",
    "Callbacks can be passed to fit, evaluate and predict methods of keras.Model. \n",
    "\n",
    "## Goals\n",
    "\n",
    "The overarching goal is to learn to use callbacks for some typical tasks. These include:\n",
    "- Reporting about training progress.\n",
    "- Stoping once training no longer reduces loss.\n",
    "- Tuning hyperparameters.\n",
    "- Implementing adaptive learning rate decay.\n",
    "- Finding an optimal batch-size for training.\n",
    "- Putting some of this into ```my_keras_utils.py``` so that they can be easily called and reused.\n",
    "\n",
    "## What's Here?\n",
    "\n",
    "I continue working with MNIST data, which I began working with in [my first Keras models](first_model.ipynb). \n",
    "\n",
    "My **concrete objective** is to tune a model that does well on Kaggle: 97th percentile? That's tough, but I think I can make it work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import my_keras_utils as my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(38000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "## Load our data.\n",
    "## Since the load process is a little slow, the try-except allows us to re-run all \n",
    "## cells without having to wait. \n",
    "try:\n",
    "    ## Raises NameError and loads data if X_train is not defined.\n",
    "    X_train.shape\n",
    "except NameError: \n",
    "    ((X_train, y_train), (X_dev, y_dev), (X_test, y_test)) = my_utils.load_kaggle_mnist()\n",
    "X_train.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A custom callback, for people who want periodic updates during training. \n",
    "\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "class TimedProgressUpdate(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Prints a progress update at time intervals during training.\n",
    "\n",
    "    The updates occur following the first completed epoch of training after which at least update_interval\n",
    "    number of minutes have passed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    update_interval: numeric\n",
    "        The number of minutes between updates. If update_interval is less than one, the fraction of a minute\n",
    "        between updates. The minimum value is 1/60 (~.016667)\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the update interval is less than 1/60, Raises a value error (updates can occur at most every one second.)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, update_interval=1):\n",
    "        super(TimedProgressUpdate, self).__init__()\n",
    "        if update_interval < 1./60:\n",
    "            error_string = \"The minimum update interval is one second. update_interval of {} implies {:.4f} seconds per update.\"\n",
    "            raise ValueError(error_string.format(update_interval, update_interval*60))\n",
    "        self.update_interval = update_interval\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = datetime.now()\n",
    "        self.last_update = self.start_time\n",
    "        print(\"Begin training of {} at {}. Progress updates every {:.1f} seconds.\"\n",
    "                .format(self.model.name, self.start_time.strftime(\"%H:%M:%S\"),self.update_interval*60)\n",
    "            )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        now = datetime.now()\n",
    "        #print((now - self.last_update).seconds)\n",
    "        if (now - self.last_update).seconds >= 60*self.update_interval:\n",
    "            print(\"Starting training on  epoch {}. Current loss is {}.\".format(epoch + 1,logs['loss']))\n",
    "            self.last_update = now\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        end = datetime.now()\n",
    "        elapsed = end - self.start_time \n",
    "        print(\"Finished fitting at {}. Elapsed time {}.\".format(end.strftime(\"%H:%M:%S\"), elapsed))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Dropout\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\nrescaling_6 (Rescaling)      (None, 784)               0         \n_________________________________________________________________\ndropout_18 (Dropout)         (None, 784)               0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 100)               78500     \n_________________________________________________________________\ndropout_19 (Dropout)         (None, 100)               0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 100)               10100     \n_________________________________________________________________\ndropout_20 (Dropout)         (None, 100)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 10)                1010      \n=================================================================\nTotal params: 89,610\nTrainable params: 89,610\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Let's use the dropout model from my \"first\" model notebook.\n",
    "\n",
    "inputs = keras.Input(shape=(784))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Dropout(rate = .05)(x)\n",
    "x = layers.Dense(100, activation='relu',)(x)\n",
    "x = layers.Dropout(rate = .15)(x)\n",
    "x = layers.Dense(100, activation='relu',)(x)\n",
    "x = layers.Dropout(rate = .15)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "model_dropout = keras.Model(inputs=inputs,outputs=outputs, name='Dropout')\n",
    "model_dropout.summary()\n",
    "\n",
    "## specify the optimizer\n",
    "## .001 is the default learning rate for Adam; \n",
    "optimizer = keras.optimizers.Adam(.001)\n",
    "\n",
    "## Compile and run once to see if everything looks right\n",
    "model_dropout.compile(optimizer=optimizer, \n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "            )\n",
    "init_weights = model_dropout.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                patience = 10, \n",
    "                                                restore_best_weights = True)\n",
    "progress_update = TimedProgressUpdate(.1)\n",
    "callbacks = [progress_update, early_stopping]\n",
    "epochs = 0\n",
    "initial_epoch = 0\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Begin training of Dropout at 11:06:09. Progress updates every 6.0 seconds.\n",
      "Starting training on  epoch 18. Current loss is 0.054818589240312576.\n",
      "Starting training on  epoch 36. Current loss is 0.02928883768618107.\n",
      "Starting training on  epoch 54. Current loss is 0.024166937917470932.\n",
      "Starting training on  epoch 72. Current loss is 0.020717803388834.\n",
      "Starting training on  epoch 90. Current loss is 0.01600858010351658.\n",
      "Starting training on  epoch 108. Current loss is 0.014169310219585896.\n",
      "Starting training on  epoch 126. Current loss is 0.013337374664843082.\n",
      "Starting training on  epoch 144. Current loss is 0.01136879250407219.\n",
      "Starting training on  epoch 162. Current loss is 0.010412385687232018.\n",
      "Finished fitting at 11:07:07. Elapsed time 0:00:57.379762.\n"
     ]
    }
   ],
   "source": [
    "if False: ## reinitialize\n",
    "    model_dropout.set_weights(init_weights)\n",
    "increment_epochs = 500\n",
    "epochs += increment_epochs\n",
    "\n",
    "history = model_dropout.fit(X_train, y_train, \n",
    "                        epochs=epochs,\n",
    "                        initial_epoch = initial_epoch,\n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=(X_dev, y_dev),\n",
    "                        callbacks = callbacks,\n",
    "                        verbose = 0)\n",
    "initial_epoch += increment_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9819999933242798,\n",
       " 0.9794999957084656,\n",
       " 0.9800000190734863,\n",
       " 0.9829999804496765,\n",
       " 0.9804999828338623,\n",
       " 0.9810000061988831,\n",
       " 0.9835000038146973,\n",
       " 0.9810000061988831,\n",
       " 0.9829999804496765,\n",
       " 0.9815000295639038,\n",
       " 0.9819999933242798,\n",
       " 0.9825000166893005,\n",
       " 0.9800000190734863,\n",
       " 0.9815000295639038,\n",
       " 0.9800000190734863,\n",
       " 0.9815000295639038,\n",
       " 0.9810000061988831,\n",
       " 0.9794999957084656,\n",
       " 0.9819999933242798,\n",
       " 0.9800000190734863]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "history.history['val_acc'][-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}