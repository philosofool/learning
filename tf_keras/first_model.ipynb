{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow",
   "display_name": "Python 3.8 (tensorflow)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# First Keras project\n",
    "\n",
    "Keeping it simple. Probably a little too simple, but let's finish this, learn what we learn and move on to something more educational.\n",
    "\n",
    "(I'm working with Kaggle's MNIST digist data. It's all preprocessed except for one little thing, which is why this is too easy.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.3.0-tf'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#tf.python.client.device_lib.list_local_devices() #verifies GPU type.\n",
    "tf.config.experimental.list_physical_devices('GPU')#verifies GPU detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: ## this means that if I re-run all cells, I don't have to wait for pd.read_csv, which is a little slow.\n",
    "    dataframe.head()\n",
    "except NameError:\n",
    "    dataframe = pd.read_csv('data/train.csv')\n",
    "dev_df = dataframe.sample(n=3000, random_state=1)\n",
    "train_df = dataframe.drop(dev_df.index)\n",
    "assert train_df.shape[1] == 785 #should be 784 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I have some learning to do with datasets. \n",
    "## So the cells below don't get used as of 16/11/20.\n",
    "\n",
    "def dataframe_to_dataset(dataframe, batch_size=64, label='label'):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dataframe.drop(label,axis=1).to_numpy(), dataframe[label]))\n",
    "    ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "#dev_ds = dataframe_to_dataset(dev_df)\n",
    "#train_ds = dataframe_to_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\nrescaling_1 (Rescaling)      (None, 784)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               78500     \n_________________________________________________________________\ndense_4 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 89,610\nTrainable params: 89,610\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "##Build the model\n",
    "inputs = keras.Input(shape=(784))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Dense(100, activation='relu')(x)\n",
    "x = layers.Dense(100, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs,outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "610/610 [==============================] - 2s 4ms/step - loss: 0.3468 - acc: 0.8996 - val_loss: 0.2098 - val_acc: 0.9390\n"
     ]
    }
   ],
   "source": [
    "## Cool. \n",
    "## Compile and train\n",
    "\n",
    "model.compile(optimizer=\"adam\", \n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "            )\n",
    "(X, y) = (train_df.drop('label',axis=1).to_numpy(), train_df['label'].to_numpy())\n",
    "(X_val, y_val) = (dev_df.drop('label', axis = 1).to_numpy(), dev_df['label'].to_numpy())\n",
    "assert X.shape[1] == 784\n",
    "assert X.shape[0] == y.shape[0]\n",
    "history = model.fit(X, y, epochs=1, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.1425 - acc: 0.9570 - val_loss: 0.1442 - val_acc: 0.9573\n",
      "Epoch 2/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0985 - acc: 0.9699 - val_loss: 0.1309 - val_acc: 0.9613\n",
      "Epoch 3/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0743 - acc: 0.9772 - val_loss: 0.1048 - val_acc: 0.9710\n",
      "Epoch 4/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0573 - acc: 0.9827 - val_loss: 0.1149 - val_acc: 0.9680\n",
      "Epoch 5/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0447 - acc: 0.9863 - val_loss: 0.1202 - val_acc: 0.9647\n",
      "Epoch 6/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0371 - acc: 0.9884 - val_loss: 0.1090 - val_acc: 0.9710\n",
      "Epoch 7/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0308 - acc: 0.9899 - val_loss: 0.1201 - val_acc: 0.9673\n",
      "Epoch 8/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.1408 - val_acc: 0.9623\n",
      "Epoch 9/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0206 - acc: 0.9930 - val_loss: 0.1523 - val_acc: 0.9630\n",
      "Epoch 10/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0206 - acc: 0.9931 - val_loss: 0.1379 - val_acc: 0.9710\n",
      "Epoch 11/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.1389 - val_acc: 0.9713\n",
      "Epoch 12/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0135 - acc: 0.9956 - val_loss: 0.1419 - val_acc: 0.9693\n",
      "Epoch 13/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0139 - acc: 0.9956 - val_loss: 0.1509 - val_acc: 0.9653\n",
      "Epoch 14/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0116 - acc: 0.9959 - val_loss: 0.1368 - val_acc: 0.9730\n",
      "Epoch 15/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.1586 - val_acc: 0.9707\n",
      "Epoch 16/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.1524 - val_acc: 0.9703\n",
      "Epoch 17/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1853 - val_acc: 0.9643\n",
      "Epoch 18/25\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 0.0133 - acc: 0.9953 - val_loss: 0.1815 - val_acc: 0.9677\n",
      "Epoch 19/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1940 - val_acc: 0.9653\n",
      "Epoch 20/25\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.1562 - val_acc: 0.9707\n",
      "Epoch 21/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.1873 - val_acc: 0.9687\n",
      "Epoch 22/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.2041 - val_acc: 0.9673\n",
      "Epoch 23/25\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 0.0070 - acc: 0.9975 - val_loss: 0.1828 - val_acc: 0.9693\n",
      "Epoch 24/25\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.1880 - val_acc: 0.9713\n",
      "Epoch 25/25\n",
      "610/610 [==============================] - 2s 3ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.1896 - val_acc: 0.9687\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc184e60ee0>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model.fit(X, y, epochs=25, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "610/610 [==============================] - 3s 4ms/step - loss: 0.0071 - acc: 0.9973 - val_loss: 0.1787 - val_acc: 0.9717\n",
      "Epoch 2/2\n",
      "610/610 [==============================] - 2s 4ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.2062 - val_acc: 0.9700\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc1850e90d0>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "## We can now babysit by re-running until there's no consistent improvement with \n",
    "## training loss.\n",
    "model.fit(X, y, epochs=2, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "source": [
    "## First ever Keras model complete.\n",
    "\n",
    "That model was doomed to overfit the data and never got consistently better after ~30 epochs. \n",
    "\n",
    "Next, I learn to add L2 regularization to reduce bias."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}